{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['▁안녕',\n",
       " '하',\n",
       " '세',\n",
       " '요.',\n",
       " '▁한국어',\n",
       " '▁G',\n",
       " 'P',\n",
       " 'T',\n",
       " '-2',\n",
       " '▁입',\n",
       " '니다.',\n",
       " '😤',\n",
       " ':)',\n",
       " 'l^o']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\", bos_token='</s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>') \n",
    "tokenizer.tokenize(\"안녕하세요. 한국어 GPT-2 입니다.😤:)l^o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "근육이 커지기 위해서는 무엇보다 규칙적인 생활습관이 중요하다.\n",
      "특히, 아침식사는 단백질과 비타민이 풍부한 과일과 채소를 많이 섭취하는 것이 좋다.\n",
      "또한 하루 30분 이상 충분한 수면을 취하는 것도 도움이 된다.\n",
      "아침 식사를 거르지 않고 규칙적으로 운동을 하면 혈액순환에 도움을 줄 뿐만 아니라 신진대사를 촉진해 체내 노폐물을 배출하고 혈압을 낮춰준다.\n",
      "운동은 하루에 10분 정도만 하는 게 좋으며 운동 후에는 반드시 스트레칭을 통해 근육량을 늘리고 유연성을 높여야 한다.\n",
      "운동 후 바로 잠자리에 드는 것은 피해야 하며 특히 아침에 일어나면 몸이 피곤해지기 때문에 무리하게 움직이면 오히려 역효과가 날 수도 있다.\n",
      "운동을\n"
     ]
    }
   ],
   "source": [
    "text = '근육이 커지기 위해서는'\n",
    "input_ids = tokenizer.encode(text)\n",
    "gen_ids = model.generate(torch.tensor([input_ids]),\n",
    "                           max_length=128,\n",
    "                           repetition_penalty=2.0,\n",
    "                           pad_token_id=tokenizer.pad_token_id,\n",
    "                           eos_token_id=tokenizer.eos_token_id,\n",
    "                           bos_token_id=tokenizer.bos_token_id,\n",
    "                           use_cache=True)\n",
    "generated = tokenizer.decode(gen_ids[0,:].tolist())\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers.optimization import AdamW, get_cosine_schedule_with_warmup\n",
    "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_TKN = \"<usr>\"\n",
    "A_TKN = \"<sys>\"\n",
    "BOS = '</s>'\n",
    "EOS = '</s>'\n",
    "MASK = '<unused0>'\n",
    "SENT = '<unused1>'\n",
    "PAD = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "koGPT2_TOKENIZER = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
    "            bos_token=BOS, eos_token=EOS, unk_token='<unk>',\n",
    "            pad_token=PAD, mask_token=MASK) \n",
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\",\n",
    "    filename=\"ChatBotData.csv\",\n",
    ")\n",
    "Chatbot_Data = pd.read_csv(\"ChatBotData.csv\")\n",
    "# Test 용으로 300개 데이터만 처리한다.\n",
    "Chatbot_Data = Chatbot_Data[:300]\n",
    "Chatbot_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotDataset(Dataset):\n",
    "    def __init__(self, chats, max_len = 40):\n",
    "        self._data = chats\n",
    "        self.max_len = max_len\n",
    "        self.q_token = Q_TKN\n",
    "        self.a_token = A_TKN\n",
    "        self.sent_token = SENT\n",
    "        self.eos = EOS\n",
    "        self.mask = MASK\n",
    "        self.tokenizer = koGPT2_TOKENIZER\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        turn = self._data.iloc[idx]  #dataframe에서 idx행에 있는 데이터\n",
    "        q = turn[\"Q\"] # 질문을 가져온다\n",
    "        q = re.sub(r\"([?.!,])\", r\" \", q)  # 구둣점들을 제거한다\n",
    "        \n",
    "        a = turn[\"A\"]  # 답변을 가져온다.\n",
    "        a = re.sub(r\"([?.!,])\", r\" \", a)  # 구둣점들을 제거한다.\n",
    "        \n",
    "        q_toked = self.tokenizer.tokenize(self.q_token + q + self.sent_token)\n",
    "        q_len = len(q_toked)\n",
    "        \n",
    "        a_toked = self.tokenizer.tokenize(self.a_token + a + self.eos)\n",
    "        a_len = len(a_toked)\n",
    "        \n",
    "        #질문의 길이가 최대 길이(gpt seq-length)보다 크면\n",
    "        if q_len > self.max_len:\n",
    "            # a_len = self.max_len - q_len   #이거 두개는 왜 넣은지 모르겠음\n",
    "            # if a_len <= 0:\n",
    "            q_toked = q_toked[-(int(self.max_len/2)):] #이거한번 출력해보기 이러면 처음 self.q_token 붙여둔건 날아가지않을까\n",
    "            q_len = len(q_toked)\n",
    "            a_len = self.max_len - q_len\n",
    "            a_token = a_toked[:a_len]    # 이러면 self.eos는 날아가지 않을까?\n",
    "            a_len = len(a_token)\n",
    "                \n",
    "        #질문의 길이 + 답변의 길이가 최대길이보다 크면\n",
    "        if q_len + a_len > self.max_len:\n",
    "            a_len = self.max_len - q_len\n",
    "            if a_len <=0:\n",
    "                q_toked = q_toked[-(int(self.max_len / 2)) :]   #질문길이를 최대길이의 반으로 \n",
    "                q_len = len(q_toked)\n",
    "                a_len = self.max_len - q_len              #답변의 길이를 최대길이 - 질문길이\n",
    "            a_toked = a_toked[:a_len]\n",
    "            a_len = len(a_toked)\n",
    "            \n",
    "            \n",
    "        # 답변 labels = [mask, mask, ...., mask, ..., <bos>,..답변.. <eos>, <pad>....]\n",
    "        labels = [self.mask,]*q_len + a_toked[1:] # a_toked와 q_toked 한번 출력해봐야 겠다.\n",
    "        \n",
    "        # mask = 질문길이 0 + 답변길이 1 + 나머지 0\n",
    "        mask = [0]*q_len + [1]*a_len + [0]*(self.max_len - q_len - a_len)\n",
    "        \n",
    "        # 답변 labels를 index로 만든다.\n",
    "        labels_ids = self.tokenizer.convert_tokens_to_ids(labels)\n",
    "        #최대길이만큼 패딩\n",
    "        while len(labels_ids) < self.max_len:  #이거 그냥 빼기로 수정하는게 좋을거 같음\n",
    "            labels_ids += [self.tokenizer.pad_token_id]\n",
    "            \n",
    "            \n",
    "        # 질문 + 답변을 index로 만든다\n",
    "        token_ids = self.tokenizer.convert_tokens_to_ids(q_toked + a_toked)\n",
    "        # 최대길이만큼 패딩\n",
    "        while len(token_ids) < self.max_len:\n",
    "            token_ids += [self.tokenizer.pad_token_id]\n",
    "            \n",
    "        #질문+답변, 마스크, 답변\n",
    "        return (token_ids, np.array(mask), labels_ids)\n",
    "    \n",
    "    # token_ids는 질문+감정+답변+pad_token_id 순이라는데 출력해보자\n",
    "    # mask는 질문 q가 들어 가는 곳에는 0, 답변 a가 위치한 곳에는 1, 빈공간은 0으로 채워진다\n",
    "    # labels는 질문의 길이만큼 mask 문자 그리고 답변 a의 id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    data = [item[0] for item in batch]\n",
    "    mask = [item[1] for item in batch]\n",
    "    label = [item[2] for item in batch]\n",
    "    return torch.LongTensor(data), torch.LongTensor(mask), torch.LongTensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = ChatbotDataset(Chatbot_Data, max_len=40)\n",
    "\n",
    "#윈도우 환경에서 num_workers 는 무조건 0으로 지정, 리눅스에서는 2\n",
    "train_dataloader = DataLoader(train_set, batch_size=32, num_workers=0, shuffle=True, collate_fn=collate_batch,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "token_ids ====>  tensor([[    2, 22311, 15495,  ...,     3,     3,     3],\n",
      "        [    2,  9244, 16905,  ...,     3,     3,     3],\n",
      "        [    2, 15403,  9038,  ...,     3,     3,     3],\n",
      "        ...,\n",
      "        [    2, 10715, 12704,  ...,     3,     3,     3],\n",
      "        [    2, 31279, 11732,  ...,     3,     3,     3],\n",
      "        [    2, 19787, 15495,  ...,     3,     3,     3]])\n",
      "mask =====>  tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "label =====>  tensor([[9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        ...,\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3]])\n",
      "token_ids ====>  tensor([[    2, 27137, 11732,  ...,     3,     3,     3],\n",
      "        [    2, 11018,  9068,  ...,     3,     3,     3],\n",
      "        [    2, 10715,  9511,  ...,     3,     3,     3],\n",
      "        ...,\n",
      "        [    2, 21435, 11630,  ...,     3,     3,     3],\n",
      "        [    2, 22311, 26089,  ...,     3,     3,     3],\n",
      "        [    2, 15403,  9355,  ...,     3,     3,     3]])\n",
      "mask =====>  tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "label =====>  tensor([[9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        ...,\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3]])\n",
      "token_ids ====>  tensor([[    2,  9099, 13692,  ...,     3,     3,     3],\n",
      "        [    2,  9228,  8078,  ...,     3,     3,     3],\n",
      "        [    2,  9086,  7194,  ...,     3,     3,     3],\n",
      "        ...,\n",
      "        [    2,  9028,  8400,  ...,     3,     3,     3],\n",
      "        [    2,  9120, 36598,  ...,     3,     3,     3],\n",
      "        [    2, 10411, 13243,  ...,     3,     3,     3]])\n",
      "mask =====>  tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "label =====>  tensor([[9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        ...,\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3]])\n",
      "token_ids ====>  tensor([[    2, 15983,  7692,  ...,     3,     3,     3],\n",
      "        [    2, 10715, 18338,  ...,     3,     3,     3],\n",
      "        [    2,   739,  6910,  ...,     3,     3,     3],\n",
      "        ...,\n",
      "        [    2, 25883, 11630,  ...,     3,     3,     3],\n",
      "        [    2, 10464, 12079,  ...,     3,     3,     3],\n",
      "        [    2,  9086,  6835,  ...,     3,     3,     3]])\n",
      "mask =====>  tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "label =====>  tensor([[9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        ...,\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3]])\n",
      "token_ids ====>  tensor([[    2, 10514,  7235,  ...,     3,     3,     3],\n",
      "        [    2,  9779, 15403,  ...,     3,     3,     3],\n",
      "        [    2, 10464, 12079,  ...,     3,     3,     3],\n",
      "        ...,\n",
      "        [    2,  9716, 11018,  ...,     3,     3,     3],\n",
      "        [    2, 20509,  7847,  ...,     3,     3,     3],\n",
      "        [    2, 11018, 12642,  ...,     3,     3,     3]])\n",
      "mask =====>  tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "label =====>  tensor([[9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        ...,\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3]])\n",
      "token_ids ====>  tensor([[    2,  9716, 10056,  ...,     3,     3,     3],\n",
      "        [    2,  9504, 16484,  ...,     3,     3,     3],\n",
      "        [    2, 23356, 10433,  ...,     3,     3,     3],\n",
      "        ...,\n",
      "        [    2, 27820,  7380,  ...,     3,     3,     3],\n",
      "        [    2,  9779, 20362,  ...,     3,     3,     3],\n",
      "        [    2, 15983,  7673,  ...,     3,     3,     3]])\n",
      "mask =====>  tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "label =====>  tensor([[9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        ...,\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3]])\n",
      "token_ids ====>  tensor([[    2, 11018,  9443,  ...,     3,     3,     3],\n",
      "        [    2, 22257, 22692,  ...,     3,     3,     3],\n",
      "        [    2, 10715, 10628,  ...,     3,     3,     3],\n",
      "        ...,\n",
      "        [    2,  9065,  7487,  ...,     3,     3,     3],\n",
      "        [    2, 17542, 33413,  ...,     3,     3,     3],\n",
      "        [    2,  9086,  8223,  ...,     3,     3,     3]])\n",
      "mask =====>  tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "label =====>  tensor([[9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        ...,\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3]])\n",
      "token_ids ====>  tensor([[    2, 15403,  9355,  ...,     3,     3,     3],\n",
      "        [    2,  9779, 13950,  ...,     3,     3,     3],\n",
      "        [    2,  9065, 36562,  ...,     3,     3,     3],\n",
      "        ...,\n",
      "        [    2, 19787,  9183,  ...,     3,     3,     3],\n",
      "        [    2, 15403, 18767,  ...,     3,     3,     3],\n",
      "        [    2, 13016,  8702,  ...,     3,     3,     3]])\n",
      "mask =====>  tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "label =====>  tensor([[9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        ...,\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3]])\n",
      "token_ids ====>  tensor([[    2, 10411, 39558,  ...,     3,     3,     3],\n",
      "        [    2, 10137,  8133,  ...,     3,     3,     3],\n",
      "        [    2,  9779,  9337,  ...,     3,     3,     3],\n",
      "        ...,\n",
      "        [    2,  9244,  6958,  ...,     3,     3,     3],\n",
      "        [    2, 11018, 11732,  ...,     3,     3,     3],\n",
      "        [    2,  9065, 22081,  ...,     3,     3,     3]])\n",
      "mask =====>  tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "label =====>  tensor([[9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        ...,\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3],\n",
      "        [9, 9, 9,  ..., 3, 3, 3]])\n",
      "token_ids ====>  tensor([[    2, 11018,  9347, 12704, 10341,  6969,    10,     4,  9067, 10101,\n",
      "         16899,  9677,  8234,   739,     1,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    2, 11018,  9154, 15254,  8135,  9673,    10,     4, 13701, 12503,\n",
      "         12821, 14807,  8084,   739,     1,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    2, 10464,  9341,   739,    10,     4,  9265,  7470,  9659,  9701,\n",
      "         11389, 11676,  7177,   739,  9265,  7380, 11120,  8711, 10764, 11389,\n",
      "          9728, 12245, 22238,  9341,  8084,     1,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    2, 17542, 12668, 13585,  7055,    10,     4, 17542,  9039, 11694,\n",
      "         24508,  8263, 11492,  8084,   739,     1,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    2, 10637, 11258,   739,  8285, 11848,    10,     4, 23943,  9025,\n",
      "          9846,  9122,  8046,  8084,   739,     1,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    2, 19787, 49542,  8263,   739,    10,     4, 11342,  9658,  9867,\n",
      "         42226, 24825,  7801,  8084,   739,     1,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    2,  9085,  7597,   395,  8149, 10624,  7397, 24224, 13358,  7182,\n",
      "            10,     4, 12079,  8135, 16899,  9677,  8234,   739,     1,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    2, 10715, 12704, 10341,  6969,    10,     4,  9018, 42018,  7182,\n",
      "         22386, 10229,  7380,  9078,  7801,  8084,   739,     1,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    2,  9546,  6969,  9135,  6969,   739,    10,     4, 47981,  8084,\n",
      "           739,     1,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    2,  9086, 11891, 10142,    10,     4, 44497, 10137,  9677,  6872,\n",
      "          7098,  8084,   739,     1,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    2,  9086, 12790,  9069,  7621,  7422,  8006,   739,    10,     4,\n",
      "         44497, 10137,  9677,  6872,  7098,  8084,   739,     1,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    2, 10715, 11469, 10855, 15570,  9625,  6969,    10,     4, 12160,\n",
      "         23142,  9161, 28024, 15106,  8149,  9122,  8046,  8084,   739,     1,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3]])\n",
      "mask =====>  tensor([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "label =====>  tensor([[    9,     9,     9,     9,     9,     9,     9,  9067, 10101, 16899,\n",
      "          9677,  8234,   739,     1,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    9,     9,     9,     9,     9,     9,     9, 13701, 12503, 12821,\n",
      "         14807,  8084,   739,     1,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    9,     9,     9,     9,     9,  9265,  7470,  9659,  9701, 11389,\n",
      "         11676,  7177,   739,  9265,  7380, 11120,  8711, 10764, 11389,  9728,\n",
      "         12245, 22238,  9341,  8084,     1,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    9,     9,     9,     9,     9,     9, 17542,  9039, 11694, 24508,\n",
      "          8263, 11492,  8084,   739,     1,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    9,     9,     9,     9,     9,     9,     9, 23943,  9025,  9846,\n",
      "          9122,  8046,  8084,   739,     1,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    9,     9,     9,     9,     9,     9, 11342,  9658,  9867, 42226,\n",
      "         24825,  7801,  8084,   739,     1,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    9,     9,     9,     9,     9,     9,     9,     9,     9,     9,\n",
      "             9, 12079,  8135, 16899,  9677,  8234,   739,     1,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    9,     9,     9,     9,     9,     9,  9018, 42018,  7182, 22386,\n",
      "         10229,  7380,  9078,  7801,  8084,   739,     1,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    9,     9,     9,     9,     9,     9,     9, 47981,  8084,   739,\n",
      "             1,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    9,     9,     9,     9,     9, 44497, 10137,  9677,  6872,  7098,\n",
      "          8084,   739,     1,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    9,     9,     9,     9,     9,     9,     9,     9,     9, 44497,\n",
      "         10137,  9677,  6872,  7098,  8084,   739,     1,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    9,     9,     9,     9,     9,     9,     9,     9, 12160, 23142,\n",
      "          9161, 28024, 15106,  8149,  9122,  8046,  8084,   739,     1,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3]])\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "print(\"start\")\n",
    "for batch_idx, samples in enumerate(train_dataloader):\n",
    "    token_ids, mask, label = samples\n",
    "    print(\"token_ids ====> \", token_ids)\n",
    "    print(\"mask =====> \", mask)\n",
    "    print(\"label =====> \", label)\n",
    "print(\"end\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
